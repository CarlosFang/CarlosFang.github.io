<?xml version="1.0" encoding="utf-8"?>
  <rss version="2.0"
        xmlns:content="http://purl.org/rss/1.0/modules/content/"
        xmlns:atom="http://www.w3.org/2005/Atom"
  >
  <channel>
    <title>xyecho</title>
    <link href="http://localhost:4000/feed/" rel="self" />
    <link href="http://localhost:4000" />
    <lastBuildDate>2018-09-10T20:27:05+08:00</lastBuildDate>
    <webMaster>1447675994@qq.com</webMaster>
    
    <item>
      <title>muduo笔记  第八章 muduo网络库设计与实现</title>
      <link href="http://localhost:4000/muduo-8-muduo-EventLoop/"/>
      <pubDate>2018-01-05T04:21:12+08:00</pubDate>
      <author>xyecho</author>
      <guid>http://localhost:4000/muduo-8-muduo-EventLoop</guid>
      <content:encoded><![CDATA[<p>runInLoop() 线程安全的理解</p>

<p><img src="/assets/muduo/8-muduo-EventLoop.png" alt="" /></p>

<p>在线程T1 muduo::EventLoop loop;， 并loop.loop();那么这个事件应该在线程T1上跑。但是线程T2做了一件事，就是loop::runInLoop(cb); 这个时候添加回调时和loop并不是同一个线程。会有问题。</p>

<p>所以,runInLoop()的做法是：先判断是不是同一个线程，是的话就直接被执行，不是的话就加到挂起队列中。</p>

<p>// 是否在当线线程，不在就加入队列。 因为可能是其他线程执行这个代码，</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">EventLoop</span><span class="o">::</span><span class="n">runInLoop</span><span class="p">(</span><span class="k">const</span> <span class="n">Functor</span> <span class="o">&amp;</span><span class="n">cb</span><span class="p">)</span>
<span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">isInLoopThread</span><span class="p">())</span>
  <span class="p">{</span>
    <span class="n">cb</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="k">else</span>
  <span class="p">{</span>
    <span class="n">queueInLoop</span><span class="p">(</span><span class="n">cb</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>在queueInLoop中会唤醒T1线程。做法是：</p>

<p>T1和T2线程有一个channel,写入一个字符。触发T1的事件，T1在loop中会执行被挂起的函数队列。</p>

<p>addTimer 要做到线程安全，就是把它的回调加入到runInLoop，这样它可以触发它所在的线程去执行。</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TimerId</span> <span class="n">TimerQueue</span><span class="o">::</span><span class="n">addTimer</span><span class="p">(</span><span class="k">const</span> <span class="n">TimerCallback</span><span class="o">&amp;</span> <span class="n">cb</span><span class="p">,</span>
                             <span class="n">Timestamp</span> <span class="n">when</span><span class="p">,</span>
                             <span class="kt">double</span> <span class="n">interval</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">Timer</span><span class="o">*</span> <span class="n">timer</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Timer</span><span class="p">(</span><span class="n">cb</span><span class="p">,</span> <span class="n">when</span><span class="p">,</span> <span class="n">interval</span><span class="p">);</span>
  <span class="n">loop_</span><span class="o">-&gt;</span><span class="n">runInLoop</span><span class="p">(</span>
      <span class="n">boost</span><span class="o">::</span><span class="n">bind</span><span class="p">(</span><span class="o">&amp;</span><span class="n">TimerQueue</span><span class="o">::</span><span class="n">addTimerInLoop</span><span class="p">,</span> <span class="k">this</span><span class="p">,</span> <span class="n">timer</span><span class="p">));</span>
  <span class="k">return</span> <span class="n">TimerId</span><span class="p">(</span><span class="n">timer</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<h4 id="84-实现tcp-网络库">8.4 实现TCP 网络库</h4>

<p>这一节主要是讲 Acceptor class。</p>

<p>Acceptor class是用于封装accept接受新连接的。为什么在单独成为一个模块类呢？</p>

<p>1.Acceptor  的成员有socket channel 。其中socket 是用了RAII handle 。</p>

<p>2.Channel用于观察此socket上的readable事件，并回调Acceptor:: handleRead()，后者会调用accept(2)来接受新连接，并回调用户callback。</p>

<p>3.关于，如果系统的fd耗尽的问题。 在一个开始，就打一个空闲的fd。当系统耗尽时，会先关闭这个空闲的fd。要给新上来的客户端accept。最后，
断开客户端，把fd交还给空闲的占用。 这么做是为了解决在系统耗尽fd时，不会断开客户端上来的链接。</p>

<h4 id="85-tcpserver--接受新连接">8.5 TcpServer  接受新连接</h4>
<hr />
<p>主要是讲TcpServer class。 tcp服务是管理accept获得TcpConnection。
这是一个新连接的接受的过程：</p>

<p><img src="/assets/muduo/8-muduo-tcpserver-class.png" alt="" /></p>

<p>TcpServer 很简单，用户只 需要设置好callback，再调用start()。就OK了。</p>

]]></content:encoded>
    </item>
    
    <item>
      <title>muduo笔记 第七章 时间轮</title>
      <link href="http://localhost:4000/muduo-7-muduo-timing-wheel/"/>
      <pubDate>2018-01-05T04:21:12+08:00</pubDate>
      <author>xyecho</author>
      <guid>http://localhost:4000/muduo-7-muduo-timing-wheel</guid>
      <content:encoded><![CDATA[<p>用timing wheel 踢掉空闲连接。</p>

<h4 id="如果一个连接连续几秒内没有收到数据就把它断开为此有两种简单粗暴的做法">如果一个连接连续几秒内没有收到数据，就把它断开，为此有两种简单、粗暴的做法：</h4>

<p>1.每个连接保存”最后收到数据的时间lastReceiveTime”， 然后用一个定时器，每秒遍历一遍所有连接， 断开那些(now - connection. lastReceiveTime) &gt; 8s 的connection。 这种做法全局只有一个repeated timer， 不过每次timeout 都要检查全部连接，如果连接数目比较大（几千上万） 这一步可能会比较费时。 ·</p>

<p>2.每个连接设置一个one-shot timer， 超时定为8s， 在超时的时候就断开本连接。当然，每次收到数据要去更新timer。 这种做法需要很多个one-shot timer， 会频繁地更新timers。如果连接数目比较大， 可能对EventLoop 的 TimerQueue 造成 压力。</p>

<p>连接超时不需要精确定时，只要大致8秒超时断开就行，多一秒、少一秒关系不大。</p>

<p>处理连接超时可用一个简单的数据结构：8个桶组成的循环队列。</p>

<p>第1个桶放1秒之后将要超时的连接，第2个桶放2秒之后将要超时的连接。每个连接一收到数据就把自己放到第8个桶，然后在每秒的timer里把第一个桶里的连接断开，把这个空桶挪到队尾。</p>

<p>这样大致可以做到8秒没有数据就超时断开连接。</p>

<h4 id="时间轮的原理">时间轮的原理</h4>

<p>简单的时间轮的基本结构是一个循环队列，还有一个指向队尾的指针（tail）。这个指针每秒移动一格，就像钟表上的时针。</p>

<p>以下是某一时刻timing wheel 的状态（ 见图7-42的左图）， 格子里的数字是倒计时（ 与通常的 timing wheel 相反）， 表示这个格子（ 桶子） 中连接的 剩余寿命。1秒以后（见图 7-42的右图）， tail 指针移动一格， 原来四点钟方向的格子被清空，其中的连接已被断开。</p>

<p><img src="/assets/muduo/7-time-wheel1.png" alt="" /></p>

<p>连接超时时被踢掉的过程
假设在某个时刻，conn1到达， 把它放到当前格子中，它的剩余寿命是7秒（见图7-43的左图）。 此后conn1 上没有收到数据。 1秒之后（见图7-43的右图），tail指向下一个格子， conn 1的剩余寿命是6秒。</p>

<p><img src="/assets/muduo/7-time-wheel2.png" alt="" /></p>

<p>又 过了几 秒， tail指向conn1之前的那个格子， conn1即将被断开（见图7-44的左图）。下一秒（见图7-44的右图），tail重新指向conn1原来所在的格子，清空其中的数据，断开conn1连接。
<img src="/assets/muduo/7-time-wheel3.png" alt="" /></p>

<h4 id="连接刷新">连接刷新</h4>

<p>如果在断开conn1之前收到数据，就把它移到当前的格子里。conn1的剩余寿命是3秒（见图7-45的左图），此时 conn 1收到数据，它的寿命恢复为7秒（见图 7-45 的右图）。</p>

<p><img src="/assets/muduo/7-time-wheel4.png" alt="" /></p>

<p>时间继续前进，conn1寿命递减，不过它已经比第一种情况长寿了（见图7-46）。</p>

<p><img src="/assets/muduo/7-time-wheel5.png" alt="" /></p>

<h4 id="多个连接">多个连接</h4>

<p>timingwheel中的每个格子是个hashset，可以容纳不止一个连接。比如一开始，conn1到达。随后，conn2到达（见图7-47），这时候tail还没有移动，两个连接位于同一个格子中，具有相同的剩余寿命。（在图7-47中画成链表，代码中是哈希表。）</p>

<hr />

<p>代码的实现体现在主要在三个地方。</p>

<p>1）新的连接上来时。</p>

<p><img src="/assets/muduo/7-time-wheel6.png" alt="" /></p>

<p>entry 为 shared_ptr类型。连接时会把entery插入到循环队列中的set当中。</p>

<p>2）当有数的数据过来时。</p>

<p><img src="/assets/muduo/7-time-wheel7.png" alt="" /></p>

<p>也会把这个连接的entry加入到循环队列中的set当中。</p>

<p>3）定时器
每一秒钟会从循环队列中拿掉一个Set。entry的shared_ptr计数就会减1。</p>

<p><img src="/assets/muduo/7-time-wheel8.png" alt="" /></p>

<p>当entry的shared_ptr被减到0时，entry就会被释放。就会断开连接。</p>

]]></content:encoded>
    </item>
    
    <item>
      <title>muduo笔记 第七章 定时器</title>
      <link href="http://localhost:4000/muduo-7-muduo-timer/"/>
      <pubDate>2018-01-05T04:20:12+08:00</pubDate>
      <author>xyecho</author>
      <guid>http://localhost:4000/muduo-7-muduo-timer</guid>
      <content:encoded><![CDATA[<p>在一般的服务端程序设计 中， 与时间有关的常见任务有：</p>

<p>1、获取当前时间， 计算时间间隔。</p>

<p>2、时区转换与日期计算； 把纽约当地时间转换为上海当地时间； 2011-02-05 之后第100天是几月几号星期几；等等。</p>

<p>3、 定时操作， 比如在预定的时间执行任务， 或者在一段延时之后执行任务。</p>

<p>计时，只使用gettimeofday(2)来获取当前时间</p>

<p>定时，只使用timerfd_*系统函数来处理定时任务</p>

<p>1、 time(2) 的精度太低了， ftime(3)被废弃。 clock_gettime(2)精度最高，但是系统调用的开销比gettimeofday大。</p>

<p>2、gettimeofday 不是系统调用，而是在用户态实现的。 没有上下文切换和陷入内核的开销。</p>

<p>3、timerfd_create(2)把时间变成了一个文件描述符，该“文件”在定时器超时的那一该变得可读，这样就能很方便的融入select(2)/poll(2)框架中，用统一的方式 来处理IO事件和超时事件，这也正是Reactor模式的长处。</p>

<p>4、传统的Reactor利用select(2)/poll(2)/epoll(4)的timeout来实现定时功能，但poll(2)和epoll_wait(2)的定时精度只有毫秒，远低于timerfd_ settime(2)的定时精度。</p>

<h4 id="muoduo-的定时器接口有三个都在eventloop中">muoduo 的定时器接口有三个。都在EventLoop中。</h4>
<p>1.runAt 在 指定的时间调用TimerCallback；</p>

<p>2.runAfter 等一段时间调用TimerCallback；</p>

<p>3.runEvery 以固定的间隔反复调用TimerCallback；</p>

<p>cancel 取消timer。 回调函数在EventLoop 对象所属的线程发生，与onMessage()、 onConnection() 等网络事件函数在同一个线程。</p>

<h3 id="注">[注]</h3>
<p>1、现在所在游戏项目中所用的时间函数是clock_gettime 然后再转成毫秒。</p>

]]></content:encoded>
    </item>
    
    <item>
      <title>muduo笔记 第七章 限制服务器的最大并发连接数</title>
      <link href="http://localhost:4000/muduo-7-muduo-max-connection/"/>
      <pubDate>2018-01-05T04:19:12+08:00</pubDate>
      <author>xyecho</author>
      <guid>http://localhost:4000/muduo-7-muduo-max-connection</guid>
      <content:encoded><![CDATA[<p>一方面， 我们不希望服务程序超载。</p>

<p>另一方面，更因为filedescriptor是 稀缺资源， 如果出现 filedescriptor耗尽，很棘手，跟”malloc() 失败/new抛出std::bad_alloc”差不多同样棘。</p>

<p>当accept(2) 返回EMFILE该如何应对？</p>

<p>这意味着本进程的文件描述符已经达到上限， 无法为新连接创建socket文件描述符。</p>

<p>但是，既然没有socket文件描述符来表示这个连接，我们就无法close(2) 它。</p>

<p>程序继续运行，回到L11再一次调用epoll_ wait。这时候epoll_wait会立刻返回，因为新连接还等待处理，listening fd还是可读的。这样程序 立刻就陷入了busy loop， CPU占用率接近100%。</p>

<p>这既影响同一event loop上的连接， 也影响同一机器上的其他服务。</p>

<h4 id="解决方法">解决方法</h4>

<p>准备一个空闲的文件描述符。 遇到这种情况，先关闭这个空闲文件， 获得一个文件描述符的名额；</p>

<p>再accept(2) 拿到新socket连接的描述符；随后立刻close(2) 它，这样就优雅地断开了客户端连接；</p>

<p>最后重新打开一个空闲文件， 把”坑”占住，以备再次出现这种情况时使用。</p>

<p>其实有另外一种比较简单的办法：</p>

<p>file descriptor是hard limit，我们可以自己设一个 稍低 一点 的 soft limit，</p>

<p>如果超过soft limit 就主动关闭新连接， 这样就可避免触及”file descriptor 耗尽” 这种边界条件。</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">errno</span> <span class="o">==</span> <span class="n">EMFILE</span><span class="p">)</span>
    <span class="p">{</span>
      <span class="o">::</span><span class="n">close</span><span class="p">(</span><span class="n">idleFd_</span><span class="p">);</span>
      <span class="n">idleFd_</span> <span class="o">=</span> <span class="o">::</span><span class="n">accept</span><span class="p">(</span><span class="n">acceptSocket_</span><span class="p">.</span><span class="n">fd</span><span class="p">(),</span> <span class="nb">NULL</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">);</span>
      <span class="o">::</span><span class="n">close</span><span class="p">(</span><span class="n">idleFd_</span><span class="p">);</span>
      <span class="n">idleFd_</span> <span class="o">=</span> <span class="o">::</span><span class="n">open</span><span class="p">(</span><span class="s">"/dev/null"</span><span class="p">,</span> <span class="n">O_RDONLY</span> <span class="o">|</span> <span class="n">O_CLOEXEC</span><span class="p">);</span>
    <span class="p">}</span>
</code></pre></div></div>

<h4 id="注">[注]</h4>
<p>1、可能我们在实践当中，中不太可能改动每个服务器的，特别当服务器越来越多时，根本不靠谱。所以在框架中解决这个问题是比较好的选择。</p>

]]></content:encoded>
    </item>
    
    <item>
      <title>muduo笔记 第七章 muduo编程示例</title>
      <link href="http://localhost:4000/muduo-7-muduo-example/"/>
      <pubDate>2018-01-05T04:18:12+08:00</pubDate>
      <author>xyecho</author>
      <guid>http://localhost:4000/muduo-7-muduo-example</guid>
      <content:encoded><![CDATA[<h4 id="为什么tcpconnectionshutdown没有直接关闭tcp连接">为什么TcpConnection::shutdown()没有直接关闭TCP连接</h4>

<p>muduo TcpConnection没有提供close()，而只提供shutdown()，这么做是为了收发数据的完整性。</p>

<p>也就是说muduo把”主动关闭连接”这件事情分成两步来做，如果要主动关闭连接，它会先关本地”写”端，等对方关闭之后，再关本地”读”端。</p>

<p>就是要把buff的数据全部发完之后才会关闭它。</p>

<h4 id="tcp分包">TCP分包</h4>
<p>在TCP这种字节流协议上做应用层分包是网络编程的基本需求。</p>

<p>分包指的是在发生一个消息（message）或一帧（frame）数据时，通过一定的处理，让接收方能从字节流中识别并截取（还原）出一个个消息。</p>

<p>“粘包问题”是个伪问题。</p>

<h4 id="muduo-buff类的设计与使用">muduo buff类的设计与使用</h4>

<p>event loop是non-blocking网络编程的核心，在现实生活中，non-blocking几乎总是和IO multiplexing一起使用，</p>

<p>原因有两点：</p>

<p>1.没有人真的会用轮询（busy-pooling）来检查某个non-blocking IO操作是否完成，这样太浪费CPU cycles。</p>

<p>2.IO multiplexing一般不能和blocking IO用在一起，因为blocking IO中<code class="highlighter-rouge">read()/write()/accept()/connect()</code>都有可能阻塞当前线程，这样线程就没办法处理其他socket上的IO事件了。</p>

<h4 id="为什么non-blocking网络编程中应用层buffer是必需的">为什么non-blocking网络编程中应用层buffer是必需的</h4>

<p>non-blocking IO的核心思想是避免阻塞在read()或write()或其他IO系统调用上，这样可以最大限度地复用thread-of-control，让一个线程能服务于多个socket连接。</p>

<p>IO线程只能阻塞在IO multiplexing函数上，如select/poll/epoll_wait。这样一来，应用层的缓冲是必需的，每个TCP socket都要有stateful的input buffer和output buffer。</p>

<h4 id="muduo-buffer的设计要点">muduo Buffer的设计要点：</h4>

<p>1.对外表现为一块连续的内存(char* p, int len)，以方便客户代码的编写。</p>

<p>2.其size()可以自动增长，以适应不同大小的消息。它不是一个fixed size array（例如char buf[8192]）。</p>

<p>3.内部以<code class="highlighter-rouge">std::vector&lt;char&gt;</code>来保存数据，并提供相应的访问函数。 Buffer其实像是一个queue，从末尾写入数据，从头部读出数据。 谁会用Buffer？谁写谁读？根据前文分析，TcpConnection会有两个Buffer成员，input buffer与output buffer。</p>

<p>4.input buffer，TcpConnection会从socket读取数据，然后写入input buffer（其实这一步是用<code class="highlighter-rouge">Buffer::readFd()</code>完成的）；客户代码从input buffer读取数据。 output buffer，客户代码会把数据写入output buffer（其实这一步是用<code class="highlighter-rouge">TcpConnection::send()</code>完成的）；TcpConnection从output buffer读取数据并写入socket。</p>

<p>5.其实，input和output是针对客户代码而言的，客户代码从input读，往output写。TcpConnection的读写正好相反。</p>

<h4 id="protobuf从消息名创建消息然后分发给不同的处理函数">protobuf从消息名创建消息，然后分发给不同的处理函数。</h4>

<h4 id="限制并发连接数">限制并发连接数</h4>

<p>一方面，我们不希望服务程序超载；另一方面，更因为filedescriptor是稀缺资源，如果出现filedescriptor耗尽，很棘手，跟“malloc()失败/new抛出std::bad_alloc”差不多同样棘手。</p>

<p>在服务端中如果在accetp中出现fd耗尽，那么就可能无法及时通知客户关闭。</p>

<p>所以，可以准备一个空闲的文件描述符。遇到这种情况，先关闭这个空闲文件，获得一个文件描述符的名额；再accept(2)拿到新socket连接的描述符；随后立刻close(2)它，这样就优雅地断开了客户端连接；最后重新打开一个空闲文件，把“坑”占住，以备再次出现这种情况时使用。</p>

<p>另一种方式，要在onConnection时统计活着的链接数。如果超过最大就shutdown。</p>

<h4 id="定时器">定时器</h4>

<p>在一般的服务端程序设计中，与时间有关的常见任务有：</p>

<p>1．获取当前时间，计算时间间隔。</p>

<p>2．时区转换与日期计算；把纽约当地时间转换为上海当地时间；2011-02-05之后第100天是几月几号星期几；等等。</p>

<p>3．定时操作，比如在预定的时间执行任务，或者在一段延时之后执行任务。</p>

<h4 id="linux时间函数">linux时间函数</h4>

<p>Linux的计时函数，用于获得当前时间：</p>

<ul>
  <li>time(2) / time_t（秒）</li>
  <li>ftime(3) / struct timeb（毫秒）</li>
  <li>gettimeofday(2) / struct timeval（微秒）</li>
  <li>clock_gettime(2) / struct timespec（纳秒）</li>
</ul>

<p>还有gmtime / localtime / timegm / mktime / strftime / struct tm等与当前时间无关的时间格式转换函数。 
定时函数，用于让程序等待一段时间或安排计划任务：</p>

<ul>
  <li>sleep(3)</li>
  <li>alarm(2)</li>
  <li>usleep(3)</li>
  <li>nanosleep(2)</li>
  <li>clock_nanosleep(2)</li>
  <li>getitimer(2) / setitimer(2)</li>
  <li>timer_create(2) / timer_settime(2) / timer_gettime(2) / timer_delete(2) ·timerfd_create(2) / timerfd_gettime(2) / timerfd_settime(2)</li>
</ul>

<h4 id="我的取舍如下">我的取舍如下：</h4>
<ul>
  <li>（计时）只使用gettimeofday(2)来获取当前时间。</li>
  <li>（定时）只使用timerfd_*系列函数来处理定时任务。</li>
</ul>

]]></content:encoded>
    </item>
    
    <item>
      <title>muduo笔记 第六章 性能评测</title>
      <link href="http://localhost:4000/muduo-6-performance-test/"/>
      <pubDate>2018-01-05T04:08:12+08:00</pubDate>
      <author>xyecho</author>
      <guid>http://localhost:4000/muduo-6-performance-test</guid>
      <content:encoded><![CDATA[<h4 id="65-性能评测">6.5 性能评测</h4>

<p>性能对比原则：采用对方的性能测试方案，用muduo实现功能相同或类似的程序，然后放到相同的软硬件环境中对比。</p>

<p>注意这里的测试只是简单地比较了平均值；其实在严肃的性能对比中至少还应该考虑分布和百分位数（percentile）的值。限于篇幅，此处从略。</p>

<p>简单地说， ping pong协议是客户端和服务器都实现echo协议。</p>

<p>当TCP连接建立时， 客户端向服务器发送一些数据， 服务器会echo回这些数据， 然后客户端再echo回服务器。这些数据就会像乒乓球一样在客户端和服务器之间来回传送， 直到有一方断开连接为止。 这是用来测试吞吐量的常用办法。</p>

<h5 id="注">[注]</h5>

<p>1.书中这里做了 asio libevent2和 muduo的对比。对于asio我没有接触过，那我就只关注  libevent2和muduo就可以了。</p>

<p>2.做对比测试时，要注明测试环境： 硬件： CPU 几核主频多少。内存多少。软件： 操作系统，版本 内核版本。(主要测框架的性能，固定这一些定量。才能做对比)</p>

<h5 id="测试方法-分了单线程测试和多线程测试-多线程测试要注意测试的cpu核数">测试方法： 分了单线程测试，和多线程测试。 多线程测试要注意测试的cpu核数。</h5>

<p>现在的CPU很快，即便是单线程单TCP连接也能把千兆以太网的带宽跑满。如果用两台机器，所有的吞吐量测试结果都将是110MiB/s，失去了对比的意义。（用Python也能跑出同样的吞吐量，或许可以对比哪个库占的CPU少。）</p>

<p>可能是由于路由器或交换机的影响，对带宽有所限制。</p>

<p>这是测试代码 ：<a href="https://gist.github.com/chenshuo/564985">https://gist.github.com/chenshuo/564985</a></p>

<p>单纯程测试结果是 muduo 比 libevent2快70%</p>

<p>跟踪libevent2的源代码发现，它每次最多从socket读取4096字节的数据（证据在buffer.c的evbuffer_ read()函数），怪不得吞吐量比muduo小很多。</p>

<p>因为在这一测试中，muduo每次读取16384字节，系统调用的性价比较高。</p>

<p>但是把缓冲区都测试成为4K，结果，muduo还是比libevent2快18%以上。</p>

<h5 id="注-1">[注]</h5>

<p>1.这里指的 单线程，缓冲区大小一样。结果还是比libevent2快18%以上。那么问题在什么地方呢？</p>

<p>2.libevent2的多线程方式并不是很好用，可能是因为这个原因，他没有做libeven2的多线程方面测试。</p>

<p>有人会说，libevent2并不是为高吞吐量的应用场景而设计的，这样的比较不公平，胜之不武。</p>

<h5 id="注-2">[注]</h5>
<p>1.那是它是专用为什么目的设计的呢？</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>muduo笔记 第六章 常见的并发网络服务器程序设计方案</title>
      <link href="http://localhost:4000/muduo-6-high-concurrency-scheme/"/>
      <pubDate>2018-01-05T04:08:12+08:00</pubDate>
      <author>xyecho</author>
      <guid>http://localhost:4000/muduo-6-high-concurrency-scheme</guid>
      <content:encoded><![CDATA[<h4 id="常见的并发网络服务器程序设计方案">常见的并发网络服务器程序设计方案：</h4>

<p><img src="/assets/muduo/high-concurrency-scheme1.png" alt="" /></p>

<h4 id="方案0">方案0</h4>

<p>一次只能服务一个链接。现在的网络服务器基本不会这么做，所以这种方案基本是废的。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/python</span>

<span class="kn">import</span> <span class="nn">socket</span>

<span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="n">client_socket</span><span class="p">,</span> <span class="n">client_address</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">client_socket</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="mi">4096</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">sent</span> <span class="o">=</span> <span class="n">client_socket</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>    <span class="c"># sendall?</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">"disconnect"</span><span class="p">,</span> <span class="n">client_address</span>
            <span class="n">client_socket</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
            <span class="k">break</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">listen_address</span> <span class="o">=</span> <span class="p">(</span><span class="s">"0.0.0.0"</span><span class="p">,</span> <span class="mi">2007</span><span class="p">)</span>
    <span class="n">server_socket</span> <span class="o">=</span> <span class="n">socket</span><span class="o">.</span><span class="n">socket</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">AF_INET</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">SOCK_STREAM</span><span class="p">)</span>
    <span class="n">server_socket</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">listen_address</span><span class="p">)</span>
    <span class="n">server_socket</span><span class="o">.</span><span class="n">listen</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="p">(</span><span class="n">client_socket</span><span class="p">,</span> <span class="n">client_address</span><span class="p">)</span> <span class="o">=</span> <span class="n">server_socket</span><span class="o">.</span><span class="n">accept</span><span class="p">()</span>
        <span class="k">print</span> <span class="s">"got connection from"</span><span class="p">,</span> <span class="n">client_address</span>
        <span class="n">handle</span><span class="p">(</span><span class="n">client_socket</span><span class="p">,</span> <span class="n">client_address</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="方案1">方案1</h4>

<p>这个方案就是一个进程服务一个客户端，　称之为child-per-client或fork()-per-client，另外也俗称process-per-connection。</p>

<p>这种方案适合并发连接数不大的情况。至今仍有一些网络服务程序用这种方式实现，比如PostgreSQL和Perforce的服务端。</p>

<p>这种方案适合”计算响应的工作量远大于fork()的开销”这种情况，比如数据库服务器。这种方案适合长连接，但不太适合短连接，因为fork()开销大于求解Sudoku的用时。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/python</span>
<span class="kn">from</span> <span class="nn">SocketServer</span> <span class="kn">import</span> <span class="n">BaseRequestHandler</span><span class="p">,</span> <span class="n">TCPServer</span>
<span class="kn">from</span> <span class="nn">SocketServer</span> <span class="kn">import</span> <span class="n">ForkingTCPServer</span><span class="p">,</span> <span class="n">ThreadingTCPServer</span>

<span class="k">class</span> <span class="nc">EchoHandler</span><span class="p">(</span><span class="n">BaseRequestHandler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span> <span class="s">"got connection from"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">client_address</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="mi">4096</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">data</span><span class="p">:</span>
                <span class="n">sent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>    <span class="c"># sendall?</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">print</span> <span class="s">"disconnect"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">client_address</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
                <span class="k">break</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">listen_address</span> <span class="o">=</span> <span class="p">(</span><span class="s">"0.0.0.0"</span><span class="p">,</span> <span class="mi">2007</span><span class="p">)</span>
    <span class="n">server</span> <span class="o">=</span> <span class="n">ForkingTCPServer</span><span class="p">(</span><span class="n">listen_address</span><span class="p">,</span> <span class="n">EchoHandler</span><span class="p">)</span>
    <span class="n">server</span><span class="o">.</span><span class="n">serve_forever</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="方案2">方案2</h4>

<p>一个线程对应一个连接。一个客户端链接上来，就会开一个线程为它服务。</p>

<p>Java网络服务多采用这种方案。它的初始化开销比方案1要小很多，但与求解Sudoku的用时差不多，仍然不适合短连接服务。</p>

<p>这种方案的伸缩性受到线程数的限制，一两百个还行，几千个的话对操作系统的scheduler恐怕是个不小的负担。</p>

<h4 id="方案3-方案4">方案3 方案4</h4>

<p>方案3是对方案1的优化。</p>

<p>方案4是对方案2的优化。</p>

<p>这两种方案是apache httpd长期使用的方案。</p>

<h4 id="方案5">方案5</h4>

<p>基本的单线程Reactor方案。 是IO复用，非阻塞的了。用注册回调可以实现网络和业务的分离。
\
这种方案的优点是由网络库搞定数据收发，程序只关心业务逻辑；缺点在前面已经谈了：适合IO密集的应用，不太适合CPU密集的应用，因为较难发挥多核的威力。</p>

<p>另外，与方案2相比，方案5处理网络消息的延迟可能要略大一些，因为方案2直接一次read(2)系统调用就能拿到请求数据，而方案5要先poll(2)再read(2)，多了一次系统调用。</p>

<h4 id="方案6">方案6</h4>

<p>是一个过渡方案。</p>

<p>收到请求之后，不在Reactor线程计算，而是创建一个新线程去计算，以充分利用多核CPU。这是非常初级的多线程应用，因为它为每个请求（而不是每个连接）创建了一个新线程。</p>

<p>这个开销可以用线程池来避免。</p>

<h4 id="方案7">方案7</h4>

<p>为每个连接创建一个计算线程，每个连接上的请求固定发给同一个线程去算，先到先得。这也是一个过渡方案，因为并发连接数受限于线程数目，这个方案或许还不如直接使用阻塞IO的thread-per-connection方案2。</p>

<h5 id="方案8">方案8</h5>

<p>为了弥补方案6中为每个请求创建线程的缺陷，我们使用固定大小线程池，程序结构如图6-12所示。全部的IO工作都在一个Reactor线程完成，而计算任务交给thread pool。如果计算任务彼此独立，而且IO的压力不大，那么这种方案是非常适用的。</p>

<p><img src="/assets/muduo/high-concurrency-scheme2.png" alt="" /></p>

<h5 id="方案9">方案9</h5>

<p>这是muduo内置的多线程方案，也是Netty内置的多线程方案。这种方案的特点是one loop per thread，有一个main Reactor负责accept(2)连接，然后把连接挂在某个sub Reactor中（muduo采用round-robin的方式来选择sub Reactor），这样该连接的所有操作都在那个sub Reactor所处的线程中完成。多个连接可能被分派到多个线程中，以充分利用CPU。</p>

<h5 id="方案10-">方案10 　</h5>

<p>这是Nginx的内置方案。如果连接之间无交互，这种方案也是很好的选择。工作进程之间相互独立，可以热升级。</p>

<h5 id="方案11-">方案11 　</h5>

<p>把方案8和方案9混合，既使用多个Reactor来处理IO，又使用线程池来处理计算。这种方案适合既有突发IO（利用多线程处理多个连接上的IO），又有突发计算的应用（利用线程池把一个连接上的计算任务分配给多个线程去做），见图6-14。</p>

<p><img src="/assets/muduo/high-concurrency-scheme3.png" alt="" /></p>

<p>一个程序到底是使用一个event loop还是使用多个event loops呢？</p>

<p>ZeroMQ的手册给出的建议是 ，按照每千兆比特每秒的吞吐量配一个event loop的比例来设置event loop的数目，即muduo::TcpServer::setThreadNum()的参数。</p>

<p>依据这条经验规则，在编写运行于千兆以太网上的网络程序时，用一个event loop就足以应付网络IO。</p>

<p>如果程序本身没有多少计算量，而主要瓶颈在网络带宽，那么可以按这条规则来办，只用一个event loop。</p>

<p>另一方面，如果程序的IO带宽较小，计算量较大，而且对延迟不敏感，那么可以把计算放到thread pool中，也可以只用一个event loop。</p>

<p>ZeroMQ的手册给出的建议 : <a href="http://zeromq.org/area:faq#toc3">http://zeromq.org/area:faq#toc3</a></p>

]]></content:encoded>
    </item>
    
    <item>
      <title>muduo笔记 第五章 高效的多线程日志</title>
      <link href="http://localhost:4000/muduo-5-threads-log/"/>
      <pubDate>2018-01-04T04:08:12+08:00</pubDate>
      <author>xyecho</author>
      <guid>http://localhost:4000/muduo-5-threads-log</guid>
      <content:encoded><![CDATA[<p><strong>“日志（logging）”有两个意思:</strong></p>

<p>1.诊断日志（diagnostic log） 　 即log4j、logback、slf4j、glog、g2log、log4cxx、log4cpp、log4cplus、Pantheios、ezlogger等常用日志库提供的日志功能。</p>

<p>2.交易日志（transaction log） 　即数据库的write-ahead log 1 、文件系统的journaling 2 等，用于记录状态变更，通过回放日志可以逐步恢复每一次修改之后的状态。</p>

<p>本章的”日志”是前一个意思，即文本的、供人阅读的日志，通常用于故障诊断和追踪（trace） ，也可用于性能分析。</p>

<p>日志通常是分布式系统中事故调查时的唯一线索，用来追寻蛛丝马迹，查出元凶。</p>

<p>在服务端编程中，日志是必不可少的，在生产环境中应该做到”Log Everything All The Time” 。对于关键进程，日志通常要记录:</p>

<ul>
  <li>收到的每条内部消息的id（还可以包括关键字段、长度、hash等）；</li>
  <li>收到的每条外部消息的全文；</li>
  <li>发出的每条消息的全文，每条消息都有全局唯一的id；</li>
  <li>关键内部状态的变更，等等。</li>
</ul>

<h4 id="c日志库的前端大体上有两种api风格">C++日志库的前端大体上有两种API风格：</h4>

<p>C/Java的<code class="highlighter-rouge">printf(fmt, ...)</code>风格，例如 <code class="highlighter-rouge">log_info("Received %d bytes from %s", len, getClientName().c_str());</code></p>

<p>C++的<code class="highlighter-rouge">stream &lt;&lt;</code> 风格，例如 <code class="highlighter-rouge">LOG_INFO &lt;&lt; "Received " &lt;&lt; len &lt;&lt; " bytes from " &lt;&lt; getClientName();</code></p>

<p>muduo日志库是C++ stream风格的另一个好处是当输出的日志级别高于语句的日志级别时，打印日志是个空操作, 运行时开销接近零。
比方说当日志级别为WARNING时，<code class="highlighter-rouge">LOG_INFO &lt;&lt;</code>是空操作，这个语句根本不会调用<code class="highlighter-rouge">std::string getClientName()</code>函数，减小了开销。而printf风格不易做到这一点。</p>

<p>可以看这篇文章 <a href="http://www.drdobbs.com/cpp/logging-in-c/201804215">Logging In C++</a></p>

<h4 id="性能需求">性能需求</h4>

<p>编写Linux服务端程序的时候，我们需要一个高效的日志库。只有日志库足够高效，程序员才敢在代码中输出足够多的诊断信息，减小运维难度，提升效率。</p>

<p>高效性体现在几方面：</p>

<ul>
  <li>每秒写几千上万条日志的时候没有明显的性能损失。</li>
  <li>能应对一个进程产生大量日志数据的场景，例如1GB/min。</li>
  <li>不阻塞正常的执行流程。</li>
  <li>在多线程程序中，不造成争用（contention）。</li>
</ul>

<p>这里列举一些具体的性能指标，考虑往普通7200rpm SATA硬盘写日志文件的情况：</p>

<p>磁盘带宽约是110MB/s，日志库应该能瞬时写满这个带宽（不必持续太久）。</p>

<p>假如每条日志消息的平均长度是110字节，这意味着1秒要写100万条日志。</p>

<p>关于日志之前写过另一篇。可以看<a href="http://blog.xyecho.com/model-log/">日志模块</a>.</p>

]]></content:encoded>
    </item>
    
    <item>
      <title>muduo笔记 第四章 c++多线程系统编程精要</title>
      <link href="http://localhost:4000/muduo-4-threads/"/>
      <pubDate>2018-01-04T04:08:12+08:00</pubDate>
      <author>xyecho</author>
      <guid>http://localhost:4000/muduo-4-threads</guid>
      <content:encoded><![CDATA[<p>学习多线程编程面临的最大的思维方式的转变有两点：</p>

<ul>
  <li>当前线程可能随时会被切换出去，或者说被抢占（preempt）了。</li>
  <li>多线程程序中事件的发生顺序不再有全局统一的先后关系 1 。</li>
</ul>

<h4 id="基本线程原语的选用">基本线程原语的选用</h4>

<p>POSIX threads的函数有110多个，真正常用的不过十几个。而且在C++程序中通常会有更为易用的wrapper，不会直接调用Pthreads函数。这11个最基本的Pthreads函数是：</p>

<ul>
  <li>2个：线程的创建和等待结束（join）。封装为muduo::Thread。</li>
  <li>4个：mutex的创建、销毁、加锁、解锁。封装为muduo::MutexLock。</li>
  <li>5个：条件变量的创建、销毁、等待、通知、广播。封装为muduo::Condition。</li>
</ul>

<p>这些封装class都很直截了当，加起来也就一两百行代码，却已经构成了多线程编程的全部必备原语。用这三样东西（thread、mutex、condition）可以完成任何多线程编程任务。</p>

<h4 id="cc系统库的线程安全性">c/c++系统库的线程安全性</h4>

<p>c++11宣言了一个线程库。（std::thread）
对于标准而言，关键的不是定义线程库，而是规定内存模型（memory model）。特别是规定一个线程对某个共享变量的修改何时能被其他线程看到，这称为内存序（memory ordering）或者内存能见度（memory visibility）。</p>

<p><strong>[这句话有三个名词：内存模型，内存有序，内存能见度。可以查找这方面的资料。]</strong></p>

<p>编写线程安全程序的一个难点在于线程安全是不可组合的（composable），一个函数foo()调用了两个线程安全的函数，而这个foo()函数本身很可能不是线程安全的。即便现在大多数glibc库函数是线程安全的，我们也不能像写单线程程序那样编写代码。</p>

<p>C++的标准库容器和std::string都不是线程安全的，只有std::allocator保证是线程安全的。一方面的原因是为了避免不必要的性能开销，另一方面的原因是单个成员函数的线程安全并不具备可组合性（composable）</p>

<h4 id="linux上的线程标识">Linux上的线程标识</h4>

<p>pthread_t并不适合用作程序中对线程的标识符。 pthread_t不一定是一个数值类型（整数或指针），也有可能是一个结构体</p>

<p>无法打印输出pthread_t，因为不知道其确切类型。也就没法在日志中用它表示当前线程的id。</p>

<p>无法比较pthread_t的大小或计算其hash值，因此无法用作关联容器的key。</p>

<p>无法定义一个非法的pthread_t值，用来表示绝对不可能存在的线程id，因此MutexLock class没有办法有效判断当前线程是否已经持有本锁。</p>

<p>pthread_t值只在进程内有意义，与操作系统的任务调度之间无法建立有效关联。比方说在/proc文件系统中找不到pthread_t对应的task。</p>

<p>在Linux上，我建议使用gettid(2)系统调用的返回值作为线程id，这么做的好处有：</p>

<p>它的类型是pid_t，其值通常是一个小整数 13 ，便于在日志中输出。</p>

<p>在现代Linux中，它直接表示内核的任务调度id，因此在/proc文件系统中可以轻易找到对应项：/proc/tid或/prod/pid/task/tid。</p>

<p>在其他系统工具中也容易定位到具体某一个线程，例如在top(1)中我们可以按线程列出任务，然后找出CPU使用率最高的线程id，再根据程序日志判断到底哪一个线程在耗用CPU。</p>

<p>任何时刻都是全局唯一的，并且由于Linux分配新pid采用递增轮回办法，短时间内启动的多个线程也会具有不同的线程id。</p>

<p>0是非法值，因为操作系统第一个进程init的pid是1。</p>

<h4 id="线程的创建与销毁的守则">线程的创建与销毁的守则</h4>

<p>线程的创建比销毁要容易得多，只需要遵循几条简单的原则：</p>

<ul>
  <li>程序库不应该在未提前告知的情况下创建自己的“背景线程”。</li>
  <li>尽量用相同的方式创建线程，例如muduo::Thread。</li>
  <li>在进入main()函数之前不应该启动线程。 ·程序中线程的创建最好能在初始化阶段全部完成。（C++保证在进入main()之前完成全局对象 ）</li>
</ul>

<h4 id="线程的销毁有几种方式-">线程的销毁有几种方式 ：</h4>

<ul>
  <li>自然死亡。从线程主函数返回，线程正常退出。</li>
  <li>非正常死亡。从线程主函数抛出异常或线程触发segfault信号等非法操作 。</li>
  <li>自杀。在线程中调用pthread_exit()来立刻退出线程。</li>
  <li>他杀。其他线程调用pthread_cancel()来强制终止某个线程。</li>
</ul>

<h4 id="善用__thread关键字">善用__thread关键字</h4>

<p>__thread使用规则27：只能用于修饰POD类型，不能修饰class类型， 因为无法自动调用构造函数和析构函数。__thread可以用于修饰全局变量、函数内的静态变量，但是不能用于修饰函数的局部变量或者class的普通成员变量。另外，__thread变量的初始化只能用编译期常量。</p>

<p>例如：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">__thread</span> <span class="n">string</span> <span class="n">t_obj1</span><span class="p">(</span><span class="s">"Chen Shou"</span><span class="p">);</span>    <span class="c1">// 错误 不能对象的构造函数</span>
<span class="kr">__thread</span> <span class="n">string</span> <span class="o">*</span> <span class="n">t_obj2</span><span class="o">=</span> <span class="n">new</span> <span class="n">string</span><span class="p">;</span>   <span class="c1">// 错误 初始化必须用编译期常量</span>
<span class="kr">__thread</span> <span class="n">string</span> <span class="o">*</span> <span class="n">t_obj3</span><span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>         <span class="c1">// 正常 但是需要手工初始化并销毁对象</span>

</code></pre></div></div>
<p>__thread变量是每个线程有一份独立实体，各个线程的变量值互不干扰。除了这个主要用途， 它还可以修饰那些”值可能会变，带有全局性，但是又不值得用全局锁保护”的变量。muduo代码中用到了 好几处__thread，简单列举如下：</p>

<ul>
  <li>muduo/base/Logging.cc 缓存最近一条日志时间的年月日时分秒，如果一秒之内输出多条日志，可避免重复格式化。另外，muduo::strerror_tl把strerror_r(3)做成如同strerror(3)一样好用，而且是线程安全的。</li>
  <li>muduo/base/ProcessINfo.cc 用线程局部变量来简化<code class="highlighter-rouge">::scandir(3)</code>的使用。</li>
  <li>muduo/base/Thread.cc 缓存每个线程的id。</li>
  <li>用于判断当前线程是否只有一个EventLoop对象。</li>
</ul>

<h4 id="多线程与io">多线程与IO</h4>

<p>为了简单起见，我认为多线程程序应该遵循的原则是：每个文件描述符只由一个线程操作，从而轻松解决消息收发的顺序性问题，也避免了关闭文件描述符的各种race condition。 一个线程可以操作多个文件描述符， 但一个线程不能操作别的线程拥有的文件描述符。 这一点不难做到， muduo网络库已经把这些细节封装了。</p>

<h4 id="raii与fork">RAII与fork()</h4>

<p>通常我们会用RAII手法来管理以上种类的资源(加锁解锁、创建销毁定时器等等).</p>

<p>但是在fork()出来的子进程中不一定正常工作，因为资源在fork()时已经被释放了。比方说用RAII技法封装timer_create()/timer_delete()，在子进程中析构函数调用timer_delete()可能会出错。</p>

<p>因为试图释放一个不存在的资源。或者更糟糕地把其他对象持有的timer给释放了（如果碰巧新建的timer_t与之重复的话）。</p>

<h4 id="多线程与fork">多线程与fork()</h4>

<p>fork()是单线程的产物。当有了多线程之后，fork()就变得有一些不适应了。</p>

<p>fork()一般不能在多线程程序中调用 ，因为Linux的fork()只克隆当前线程的thread of control，不克隆其他线程。</p>

<p>fork()之后，除了当前线程之外，其他线程都消失了。也就是说不能一下子fork()出一个和父进程一样的多线程子进程。</p>

<p>Linux没有forkall()这样的系统调用，forkall()其实也是很难办的（从语意上），因为其他线程可能等在condition。</p>

<p>所以最好不要在多线程当中调用fork(),除非你调用后马上调用exec()。彻底断了子进程和父进程的关系。</p>

<h4 id="多线程与signal">多线程与signal</h4>

<p>在多线程程序中， 使用signal的第一原则是不要使用signal。</p>

<p>关于多线程与signal关系。可以看<a href="http://blog.xyecho.com/linux-signal/">linux signal and threads</a></p>

]]></content:encoded>
    </item>
    
    <item>
      <title>muduo笔记 第三章 多线程服务器的适用场合与常用编程模型</title>
      <link href="http://localhost:4000/muduo-3-threads/"/>
      <pubDate>2018-01-04T03:08:12+08:00</pubDate>
      <author>xyecho</author>
      <guid>http://localhost:4000/muduo-3-threads</guid>
      <content:encoded><![CDATA[<p>“网络应用程序” 的基本功能可以简单归纳为”收到数据，算一算，再发出去”。</p>

<h4 id="单线程服务器的常用编程模型">单线程服务器的常用编程模型</h4>

<p>在高性能的网络程序中，使用得最为广泛的是“non-blocking IO＋IO multiplexing”这种模型，即Reactor模式。</p>

<ul>
  <li>lighttpd，单线程服务器。（Nginx与之类似，每个工作进程有一个event loop。）</li>
  <li>libevent，libev。</li>
  <li>ACE，Poco C++ libraries。</li>
  <li>Java NIO，包括Apache Mina和Netty。</li>
  <li>POE（Perl）。</li>
  <li>Twisted（Python）。</li>
</ul>

<p>在”non-blocking IO＋IO multiplexing”这种模型中，程序的基本结构是一个事件循环（event loop），以事件驱动（event-driven）和事件回调的方式实现业务逻辑：</p>

<p><img src="/assets/muduo/3-threads.png" alt="" /></p>

<p><strong>gethostbyname(3)是阻塞的，对陌生域名解析的耗时可长达数秒。</strong></p>

<p>基于事件驱动的编程模型也有其本质的缺点，它要求事件回调函数必须是非阻塞的。对于涉及网络IO的请求响应式协议，它容易割裂业务逻辑，使其散布于多个回调函数之中，相对不容易理解和维护。现代的语言有一些应对方法（例如coroutine 协程），但是本书只关注C++这种传统语言，因此就不展开讨论了。<strong>[后继可以看看协程方面技术和GO的]</strong></p>

<h4 id="多线程服务器的常用编程模型">多线程服务器的常用编程模型</h4>

<h5 id="1-one-loop-per-thread">1 One loop per thread</h5>

<p>此种模型下，程序里的每个IO线程有一个event loop（或者叫Reactor），用于处理读写和定时事件（无论周期性的还是单次的）。</p>

<p>这种方式的好处是：</p>

<ul>
  <li>线程数目基本固定，可以在程序启动的时候设置，不会频繁创建与销毁。</li>
  <li>可以很方便地在线程间调配负载。</li>
  <li>IO事件发生的线程是固定的，同一个TCP连接不必考虑事件并发。</li>
</ul>

<h5 id="2-线程池">2 线程池</h5>

<h5 id="3-推荐的服务模式">3 推荐的服务模式</h5>

<p>推荐的C++多线程服务端编程模式为：one (event) loop per thread+ thread pool。</p>

<h5 id="4-进程间通信只用tcp">4 进程间通信只用TCP</h5>

<p>Linux下进程间通信（IPC）的方式数不胜数，有：匿名管道（pipe）、具名管道（FIFO）、POSIX消息队列、共享内存、信号（signals），Sockets。其实贵精不贵多，认真用熟其中几种就可以。之所以选择socket是因为它可以跨主机，具有伸缩性。另外：</p>

<p>1.程序上意外退出， 不会组系统留下垃圾，程序重启之后比较容易地恢复。</p>

<p>2.port是独占，可以防止程序重复启动，避免造成意料之外的结果。</p>

<p>3.当一个进程崩溃了，操作系统会关闭连接，另一个进程可立刻感知。，可快速处理。（有心跳的情况）</p>

<p>4.还可以跨语言。</p>

<p>5.会涉及用字节流方式通信。会有marshal/unmarshal的开销。</p>

<p>6.用tcp的好处。容易定位分布式系统中的服务之前的依赖关系，<code class="highlighter-rouge">netstat -tpna | grep :port</code> 二通过接收和发送队列的长度也较容易定位网络 或程序故障。</p>

<p>在正常运行的时候，netstat打印的Recv-Q和Send-Q都应该接近0，或者在0附近摆动。</p>

<p>如果Recv-Q保持不变或持续增加，则通常意味着服务进程的处理速度变慢，可能发生了死锁或阻塞。</p>

<p>如果Send-Q保持不变或持续增加，有可能是对方服务器太忙、来不及处理，也有可能是网络中间某个路由器或交换机故障造成丢包，甚至对方服务器掉线，</p>

<p>这些因素都可能表现为数据发送不出去。通过持续监控Recv-Q和Send-Q就能及早预警性能或可用性故障。以下是服务端线程阻塞造成Recv-Q和客户端Send-Q激增的例子。</p>

<h4 id="多线程服务的适用应用">多线程服务的适用应用</h4>

<p>开发服务端程序的一个基本任务是处理并发连接，现在服务端网络编程处理并发连接主要有两种方式：</p>

<p>当”线程”很廉价时，一台机器上可以创建远高于CPU数目的”线程”。这时一个线程只处理一个TCP连接（甚至半个），通常使用阻塞IO（至少看起来如此）。例如，Python gevent、Go goroutine、</p>

<p>Erlang actor。这里的”线程”由语言的runtime自行调度，与操作系统线程不是一回事。（也就是说协程做并发是可以用阻塞IO的）</p>

<p>当线程很宝贵时，一台机器上只能创建与CPU数目相当的线程。这时一个线程要处理多个TCP连接上的IO，通常使用非阻塞IO和IO multiplexing。例如，libevent、muduo、Netty。这是原生线程，能被操作系统的任务调度器看见。</p>

<h4 id="必须用单线程的场合">必须用单线程的场合</h4>

<p>有两种场合必须使用单线程：</p>

<ul>
  <li>程序可能会fork</li>
  <li>限制程序的CPU占用率</li>
</ul>

<h5 id="单线程程序的优缺点">单线程程序的优缺点：</h5>

<p>单线程程序的优势：简单。程序的结构一般可是一个基于IO multiplexing的event loop。或者直接用阻塞IO。event loop也有一个明显的缺点：非抢占的。</p>

<p>多线程没有什么绝对意义上的性能优势。 这句话是说，如果用很少的CPU负载就能让IO跑满，或者用很少的IO流量就能让CPU跑满，那么多线程没啥用处。</p>

<p>多线程的适用场景是：提高响应速度，让IO和”计算”相互重叠，降低latency。虽然多线程不能提高绝对性能，但能提高平均响应性能。</p>

<h4 id="线程的分类">线程的分类</h4>

<p>一个多线程服务程序中的线程大致可分为3类：</p>

<p>1.IO线程，这类线程的主循环是IO multiplexing，阻塞地等在select/poll/epoll_wait系统调用上。这类线程也处理定时事件。当然它的功能不止IO，有些简单计算也可以放入其中，比如消息的编码或解码。</p>

<p>2.计算线程，这类线程的主循环是blockingqueue，阻塞地等在conditionvariable上。这类线程一般位于thread pool中。这种线程通常不涉及IO，一般要避免任何阻塞操作。</p>

<p>3.第三方库所用的线程，比如logging，又比如database connection。</p>

<h4 id="linux-能同时启动多少个线程">Linux 能同时启动多少个线程</h4>

]]></content:encoded>
    </item>
    
  </channel>
</rss>
